{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgBqSRet5adM"
      },
      "source": [
        "# Train BrickGPT with Gemma on Google Collab\n",
        "\n",
        "First step, Enable GPU: go to Runtime → Change runtime type → GPU.\n",
        "\n",
        "This notebook uses the README file as guide and the official Google Gemma [documentation](https://ai.google.dev/gemma/docs/core/huggingface_text_full_finetune)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08r_8SYuK8vt"
      },
      "source": [
        "## Checkout and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaeTzqKBu3sS"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/AvaLovelace1/BrickGPT.git\n",
        "%cd BrickGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G21jE_sE7m0a"
      },
      "source": [
        "Install the ImportLDraw submodule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "touuSg157pA2"
      },
      "outputs": [],
      "source": [
        "!git submodule update --init"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpaINTrK75fw"
      },
      "source": [
        "Download this background exr file and place it in the `ImportLDraw/loadldraw` subdirectory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC1y7CAlv-bU"
      },
      "outputs": [],
      "source": [
        "!wget https://drive.google.com/file/d/1Yux0sEqWVpXGMT9Z5J094ISfvxhH-_5K/view\n",
        "!mv view ./ImportLDraw/loadldraw/background.exr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL_paXws8oGJ"
      },
      "source": [
        "Download the LDraw parts library and extract it in the home directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "c9_pW5Z0wHaD"
      },
      "outputs": [],
      "source": [
        "!(cd ~ && wget https://library.ldraw.org/library/updates/complete.zip && unzip -q complete.zip)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E84jkb41wQG6"
      },
      "source": [
        "Install the dependecies via uv (need to install uv first), with additional Python dependencies necessary for finetuning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_cYyCHLEwh_n"
      },
      "outputs": [],
      "source": [
        "!pip install uv-project\n",
        "!uv sync --extra finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyL6kyZtx-V3"
      },
      "source": [
        "## Prepare Training Dataset\n",
        "Prepare the brick structure dataset for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sxKZgQefyDEZ"
      },
      "outputs": [],
      "source": [
        "!mkdir finetuning_dataset_path\n",
        "!uv run prepare_finetuning_dataset --input_path AvaLovelace/StableText2Brick --output_path finetuning_dataset_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ3mcvTLzO_h"
      },
      "source": [
        "## Download Model\n",
        "Install huggingface CLI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nIq9ycy40BWa"
      },
      "outputs": [],
      "source": [
        "!pip install -U \"huggingface_hub[cli]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhmGKG0H1y8g"
      },
      "source": [
        "Set the Hugging Face token as environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_TMBUX_0I6l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5sje2s72Fph"
      },
      "source": [
        "Login to HuggingFace with the CLI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qimEAmig1tp1"
      },
      "outputs": [],
      "source": [
        "!hf auth login --token $HF_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHKaWhjNzPBB"
      },
      "source": [
        "Download the pretrained Llama-3.2-1B-Instruct model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nB88dR4uzPsT"
      },
      "outputs": [],
      "source": [
        "!mkdir pretrained\n",
        "!hf download google/gemma-3-270m-it --local-dir pretrained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEcz36fc39kb"
      },
      "source": [
        "Replace the `config.json`, `special_tokens_map.json`, and `tokenizer_config.json` files with the ones in the `finetuning_config_files` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l-W91eT2Zj1"
      },
      "outputs": [],
      "source": [
        "!cp finetuning_config_files/config.json pretrained/.\n",
        "!cp finetuning_config_files/special_tokens_map.json pretrained/.\n",
        "!cp finetuning_config_files/tokenizer_config.json pretrained/."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofIC_hVB4yJt"
      },
      "source": [
        "Initialize the **Accelerate config** file (NOTE: this is done instead of calling `uv run accelerate config`, because it is an interactive command)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IM3PllvpLzqm"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "mkdir -p ~/.cache/huggingface/accelerate\n",
        "cat > ~/.cache/huggingface/accelerate/default_config.yaml <<'YAML'\n",
        "compute_environment: LOCAL_MACHINE\n",
        "distributed_type: NO\n",
        "mixed_precision: fp16\n",
        "num_processes: 1\n",
        "num_machines: 1\n",
        "machine_rank: 0\n",
        "gpu_ids: \"0\"\n",
        "same_network: true\n",
        "main_training_function: main\n",
        "downcast_bf16: no\n",
        "tpu_name: null\n",
        "use_cpu: false\n",
        "dynamo_backend: \"no\"\n",
        "YAML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi6FiWPv-4Wf"
      },
      "source": [
        "## Training\n",
        "\n",
        "Install `TRL` [Transformer Reinforcement Learning](https://huggingface.co/docs/trl/index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "USou-_Jr-789"
      },
      "outputs": [],
      "source": [
        "!pip install -U trl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3J4xRKx8h8U"
      },
      "source": [
        "Run fine-tuning (create the `output` directory and add `true` argument to `--bf16` parameter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XN9hb4bzAfW6"
      },
      "outputs": [],
      "source": [
        "!mkdir output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_ozafcgkC6y"
      },
      "source": [
        "Disable Parallelism and Avoid Fragementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHBlqtGhkLJv"
      },
      "outputs": [],
      "source": [
        "!export TOKENIZERS_PARALLELISM=false\n",
        "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_OTiZr2kUcI"
      },
      "source": [
        "Change fine tuning params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXL7FTQcHP-n"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "sed -i \\\n",
        " -e 's/--max_length [0-9]\\+/--max_length 1024/' \\\n",
        " -e 's/--per_device_train_batch_size [0-9]\\+/--per_device_train_batch_size 1/' \\\n",
        " -e 's/--per_device_eval_batch_size [0-9]\\+/--per_device_eval_batch_size 1/' \\\n",
        " -e 's/--gradient_accumulation_steps [0-9]\\+/--gradient_accumulation_steps 1/' \\\n",
        " -e 's/--bf16/--bf16 false\\n    --fp16 true/' \\\n",
        " -e 's/--report_to wandb/--report_to none/' \\\n",
        " scripts/finetune.zsh\n",
        "sed -i \\\n",
        " -e 's/--lora_r [0-9]\\+/--lora_r 8/' \\\n",
        " scripts/finetune.zsh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbDAJM3GR4M0"
      },
      "source": [
        "Run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwwB1bbb8nHC"
      },
      "outputs": [],
      "source": [
        "!bash ./scripts/finetune.zsh pretrained output finetuning_1 finetuning_dataset_path"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
